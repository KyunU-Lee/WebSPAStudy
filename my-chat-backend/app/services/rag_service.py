# app/services/rag_service.py
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
import os
from langchain_community.document_loaders import PyMuPDFLoader
import requests

class RAGService:
    def __init__(self, windows_ip):
        self.windows_ip = f"http://{windows_ip}:11434"
        
# [ì¶”ê°€] ì—°ê²° í…ŒìŠ¤íŠ¸ ë¡œì§
        try:
            response = requests.get(self.windows_ip)
            if response.status_code == 200:
                print(f"âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ: {self.windows_ip}")
        except:
            print(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨! IPì™€ ìœˆë„ìš° ë°©í™”ë²½ì„ í™•ì¸í•˜ì„¸ìš”.")

        self.embeddings = OllamaEmbeddings(
            model="mxbai-embed-large",
            base_url=self.windows_ip
        )
        
        self.llm = OllamaLLM(
            model="gemma3:12b",
            base_url=self.windows_ip,
            num_ctx=16384
        )
        self.vector_store = None

    def ingest_multiple_pdfs(self, file_paths: list):
        all_docs = []
        
        for file_path in file_paths:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))
            full_path = os.path.join(project_root, file_path)
            
            if os.path.exists(full_path):
                loader = PyMuPDFLoader(full_path)
                
                data = loader.load()

                total_chars = sum(len(page.page_content) for page in data)
                print(f"{file_path}: {len(data)} page, total text length  : {total_chars}")
                print(f"ğŸ“„ {file_path} ë¡œë“œ ì™„ë£Œ")
                all_docs.extend(loader.load())
            else:
                print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {full_path}")

        # ëª¨ë“  ë¬¸ì„œ í•©ì³ì„œ í•œ ë²ˆì— ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=100,
            separators=["\nì œ", "\n\n", "\n", " "]
        )
        splits = text_splitter.split_documents(all_docs)
        print(f"âœ‚ï¸ ì´ {len(splits)} ê°œì˜ ì¡°ê°ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # [ì¤‘ìš”] ê¸°ì¡´ DBê°€ ìˆë‹¤ë©´ ì‚­ì œí•˜ê±°ë‚˜ ì´ˆê¸°í™” í›„ ìƒì„±
        if os.path.exists("./chroma_db"):
            import shutil
            shutil.rmtree("./chroma_db") # ê¹¨ë—í•˜ê²Œ ìƒˆë¡œ ì‹œì‘

        # ë‹¨ í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ëª¨ë“  ë²¡í„° ìƒì„±
        self.vector_store = Chroma.from_documents(
            documents=splits, 
            embedding=self.embeddings,
            persist_directory="./chroma_db"
        )
        return "í•™ìŠµ ì™„ë£Œ"

    async def get_rag_response_stream(self, question: str, systemPrompt: str):
        """ì§ˆë¬¸ì— ëŒ€í•´ ë‚´ê·œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.vector_store:
            yield "ë¨¼ì € ë¬¸ì„œë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”."

        fullSystemPrompt = f"""{systemPrompt}
        [ë¬¸ë§¥]
        {{context}}
        """

        prompt = ChatPromptTemplate.from_messages([
            ("system", fullSystemPrompt),
            ("human", "{input}") # create_stuff_documents_chainì€ ê¸°ë³¸ì ìœ¼ë¡œ 'input'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        ])
        # 2. RAG ì²´ì¸ êµ¬ì„±
        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 15}) # ê´€ë ¨ ì¡°í•­ 3ê°œ ê²€ìƒ‰
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # 3. ë‹µë³€ ìƒì„±
        # response = await rag_chain.ainvoke({"input": question})
        async for chunk in rag_chain.astream({"input": question}):
            if "answer" in chunk:
                yield chunk['answer']

        # return response["answer"]
