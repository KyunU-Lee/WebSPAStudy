# app/services/rag_service.py
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
import os
from langchain_community.document_loaders import PyMuPDFLoader
import requests

class RAGService:
    def __init__(self, windows_ip):
        self.windows_ip = f"http://{windows_ip}:11434"
        
# [ì¶”ê°€] ì—°ê²° í…ŒìŠ¤íŠ¸ ë¡œì§
        try:
            response = requests.get(self.windows_ip)
            if response.status_code == 200:
                print(f"âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ: {self.windows_ip}")
        except:
            print(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨! IPì™€ ìœˆë„ìš° ë°©í™”ë²½ì„ í™•ì¸í•˜ì„¸ìš”.")

        self.embeddings = OllamaEmbeddings(
            model="mxbai-embed-large",
            base_url=self.windows_ip
        )
        
        self.llm = OllamaLLM(
            model="gemma3:12b",
            base_url=self.windows_ip,
            num_ctx=16384
        )
        self.vector_store = None

    def ingest_multiple_pdfs(self, file_paths: list):
        all_docs = []
        
        for file_path in file_paths:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))
            full_path = os.path.join(project_root, file_path)
            
            if os.path.exists(full_path):
                loader = PyMuPDFLoader(full_path)
                
                data = loader.load()

                total_chars = sum(len(page.page_content) for page in data)
                print(f"{file_path}: {len(data)} page, total text length  : {total_chars}")
                print(f"ğŸ“„ {file_path} ë¡œë“œ ì™„ë£Œ")
                all_docs.extend(loader.load())
            else:
                print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {full_path}")

        # ëª¨ë“  ë¬¸ì„œ í•©ì³ì„œ í•œ ë²ˆì— ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=100,
            separators=["\nì œ", "\n\n", "\n", " "]
        )
        splits = text_splitter.split_documents(all_docs)
        print(f"âœ‚ï¸ ì´ {len(splits)} ê°œì˜ ì¡°ê°ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # [ì¤‘ìš”] ê¸°ì¡´ DBê°€ ìˆë‹¤ë©´ ì‚­ì œí•˜ê±°ë‚˜ ì´ˆê¸°í™” í›„ ìƒì„±
        if os.path.exists("./chroma_db"):
            import shutil
            shutil.rmtree("./chroma_db") # ê¹¨ë—í•˜ê²Œ ìƒˆë¡œ ì‹œì‘

        # ë‹¨ í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ëª¨ë“  ë²¡í„° ìƒì„±
        self.vector_store = Chroma.from_documents(
            documents=splits, 
            embedding=self.embeddings,
            persist_directory="./chroma_db"
        )
        return "í•™ìŠµ ì™„ë£Œ"

    async def get_rag_response_stream(self, question: str):
        """ì§ˆë¬¸ì— ëŒ€í•´ ë‚´ê·œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.vector_store:
            yield "ë¨¼ì € ë¬¸ì„œë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”."

        # 1. í”„ë¡¬í”„íŠ¸ ì„¤ì • (í˜ë¥´ì†Œë‚˜ ë¶€ì—¬)
        system_prompt = (
            "ë‹¹ì‹ ì€ ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ë¬¸ë§¥]ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n"
            "ë°˜ë“œì‹œ ë‹µë³€ ëì— 'ì°¸ì¡° ë¬¸ì„œ' ì„¹ì…˜ì„ ë§Œë“¤ì–´ ì‚¬ìš©í•œ ë¬¸ì„œì˜ ì´ë¦„ê³¼ í˜ì´ì§€ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.\n\n"
            "### ë‹µë³€ ê·œì¹™:\n"
            "1. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¡°í•­(ì˜ˆ: ì œ10ì¡°)ì´ ë¬¸ë§¥ì— ì¡°ê¸ˆì´ë¼ë„ ì–¸ê¸‰ëœë‹¤ë©´ ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´.\n"
            "2. ì—¬ëŸ¬ ë¬¸ì„œì— ë‚´ìš©ì´ í©ì–´ì ¸ ìˆë‹¤ë©´ ì´ë¥¼ ì¢…í•©í•´ì„œ ëŒ€ë‹µí•´.\n"
            "3. ë¬¸ì„œ ë²ˆí˜¸ë‚˜ ì¡°í•­ ì œëª©(ã€ ã€‘)ì´ ë³´ì´ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•´ì¤˜.\n"
            "4. ë§Œì•½ ì •ë§ë¡œ ë‚´ìš©ì´ ì—†ë‹¤ë©´, ê´€ë ¨ëœ ìœ ì‚¬í•œ ì¡°í•­ì´ë¼ë„ ì°¾ì•„ì„œ ì•ˆë‚´í•´ì¤˜.\n\n"
            "5. ë‹µë³€ ë³¸ë¬¸ì—ì„œë„ ê°€ëŠ¥í•˜ë©´ '[ë¬¸ì„œëª…, p.00]' í˜•íƒœë¡œ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.\n"
            "[ë¬¸ë§¥]\n{context}"
        )
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])

        # 2. RAG ì²´ì¸ êµ¬ì„±
        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 15}) # ê´€ë ¨ ì¡°í•­ 3ê°œ ê²€ìƒ‰
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # 3. ë‹µë³€ ìƒì„±
        response = await rag_chain.ainvoke({"input": question})
        return response["answer"]